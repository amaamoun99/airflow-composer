from datetime import datetime
from airflow import DAG
from airflow.providers.google.cloud.transfers.postgres_to_gcs import PostgresToGCSOperator
from airflow.providers.google.cloud.transfers.gcs_to_bigquery import GCSToBigQueryOperator

# Airflow connection IDs
PG_CONN_ID = "applai_postgres_db"

# GCS bucket
GCS_BUCKET = "db-to-datalake"

# BigQuery dataset + table
BQ_PROJECT = "applai-dwh"
BQ_DATASET = "airflow_landing"
BQ_TABLE = "Order"

with DAG(
    dag_id="postgres_to_bigquery_pipeline",
    start_date=datetime(2025, 12, 1),
    schedule_interval="@daily",
    catchup=False,
) as dag:

    export = PostgresToGCSOperator(
        task_id="export_postgres_to_gcs",
        postgres_conn_id=PG_CONN_ID,
        sql="SELECT * FROM your_schema.your_table;",  
        bucket=GCS_BUCKET,
        filename="order_{{ ds_nodash }}.json",
        export_format="json",
    )

    load = GCSToBigQueryOperator(
        task_id="load_gcs_to_bq",
        bucket=GCS_BUCKET,
        source_objects=["order_{{ ds_nodash }}.json"],
        destination_project_dataset_table=f"{BQ_PROJECT}.{BQ_DATASET}.{BQ_TABLE}",
        autodetect=True,
        write_disposition="WRITE_TRUNCATE",
    )

    export >> load